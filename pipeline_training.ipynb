{
 "cells": [
  {
   "cell_type": "code",
   "id": "143fedbf29b03547",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pywt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import pickle\n",
    "\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "warnings.filterwarnings(\"ignore\", message=\"X has feature names, but SVC was fitted without feature names\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Load features and labels",
   "id": "fbee4d4350ce9b37"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load training data\n",
    "\n",
    "train_features_step_1 = pd.read_csv('data/top_5000_std_step_1_filtered.csv', index_col=0)\n",
    "train_features_step_2 = pd.read_csv('data/top_5000_std_step_2_filtered.csv', index_col=0)\n",
    "train_features_step_3 = pd.read_csv('data/top_5000_std_step_3_filtered.csv', index_col=0)\n",
    "\n",
    "training_anno_step_1 = pd.read_csv('data/step_1_training_anno.csv', index_col=0)\n",
    "training_anno_step_2 = pd.read_csv('data/step_2_training_anno.csv', index_col=0)\n",
    "training_anno_step_3 = pd.read_csv('data/step_3_training_anno.csv', index_col=0)\n"
   ],
   "id": "f0233cebd12b784e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Logistic Regression",
   "id": "7993497dd357305b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import optuna\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, confusion_matrix\n",
    "from sklearn.decomposition import PCA\n",
    "from math import sqrt\n",
    "\n",
    "# Optuna optimization function using LR\n",
    "def objective_lr(trial, features, labels, trial_data):\n",
    "\n",
    "    # print('------------------------------------------------------------------------')\n",
    "    # print(f'Training model number: {trial.number} ...') # Use trial.number\n",
    "\n",
    "    # --- Logistic Regression Hyperparameter Grid ---\n",
    "    param_grid = {\n",
    "        # PCA/Data selection params (kept from original)\n",
    "        \"n_components\": trial.suggest_categorical(\"n_components\", [ 3, 5, 7, 10, 13, 15, 20, 25]), # Use features.shape[1] for max components\n",
    "        # --- Logistic Regression specific params ---\n",
    "        \"solver\": trial.suggest_categorical(\"solver\", [\"liblinear\", \"saga\", \"lbfgs\"]), # Common solvers\n",
    "        \"penalty\": trial.suggest_categorical(\"penalty\", [\"l1\", \"l2\", None]), # Regularization types\n",
    "        \"C\": trial.suggest_float(\"C\", 1e-4, 1e2, log=True), # Inverse regularization strength\n",
    "        \"max_iter\": trial.suggest_int(\"max_iter\", 50, 1000, log=True), # Max iterations for solver\n",
    "        \"class_weight\": trial.suggest_categorical(\"class_weight\", [None, \"balanced\"]), # Handle class imbalance\n",
    "    }\n",
    "\n",
    "    # --- Solver/Penalty Compatibility Check ---\n",
    "    solver = param_grid[\"solver\"]\n",
    "    penalty = param_grid[\"penalty\"]\n",
    "\n",
    "    if solver == \"liblinear\" and penalty not in [\"l1\", \"l2\"]:\n",
    "        raise optuna.TrialPruned()\n",
    "    if solver == \"lbfgs\" and penalty not in [\"l2\", None]:\n",
    "         raise optuna.TrialPruned()\n",
    "    # 'saga' supports 'l1', 'l2', 'none' (and 'elasticnet' if we added it)\n",
    "    if penalty is None and solver not in [\"lbfgs\", \"saga\"]: # Technically newton-cg also supports none\n",
    "        # Allow saga, prune others if 'none' is selected but solver isn't compatible\n",
    "        if solver != \"saga\":\n",
    "             raise optuna.TrialPruned()\n",
    "\n",
    "\n",
    "    if param_grid['n_components'] == 0:\n",
    "        training_features_pca = features.to_numpy() # Assuming features is a DataFrame\n",
    "        training_labels = np.array(labels)\n",
    "    else:\n",
    "        # Prepare PCA features\n",
    "        pca_model_lr = PCA(n_components=param_grid['n_components'], random_state=42)\n",
    "\n",
    "        # PCA Fit & Transform\n",
    "        # Ensure features is suitable for PCA (e.g., numerical, scaled if necessary)\n",
    "        pca_model_lr.fit(features)\n",
    "        training_features_pca = pca_model_lr.transform(features)\n",
    "        training_labels = np.array(labels)\n",
    "\n",
    "    # --- Cross-Validation and Model Training ---\n",
    "    train_accuracy = 0\n",
    "    validation_accuracy = 0\n",
    "    gmean = 0\n",
    "    fold_training_accuracies = []\n",
    "    fold_validation_accuracies = []\n",
    "\n",
    "    kf = StratifiedShuffleSplit(n_splits=5, test_size=0.25, random_state=42)\n",
    "    for fold, (train_index, val_index) in enumerate(kf.split(training_features_pca, training_labels)):\n",
    "        X_train, X_val = training_features_pca[train_index], training_features_pca[val_index]\n",
    "        y_train, y_val = training_labels[train_index], training_labels[val_index]\n",
    "\n",
    "        # --- Instantiate and Train Logistic Regression Model ---\n",
    "        # Handle penalty='none' case where C is not used\n",
    "        lr_model_params = {\n",
    "            \"solver\": param_grid['solver'],\n",
    "            \"penalty\": param_grid['penalty'],\n",
    "            \"max_iter\": param_grid['max_iter'],\n",
    "            \"class_weight\": param_grid['class_weight'],\n",
    "            \"random_state\": 42,\n",
    "            \"n_jobs\": 1 # Use all available CPU cores\n",
    "        }\n",
    "        if param_grid['penalty'] is not None:\n",
    "            lr_model_params[\"C\"] = param_grid['C']\n",
    "        # If penalty is 'none', solver must be compatible (checked above) and C is ignored by sklearn\n",
    "\n",
    "        # Handle solver specific requirements if penalty is 'none'\n",
    "        if param_grid['penalty'] is None and param_grid['solver'] == 'liblinear':\n",
    "             # liblinear does not support penalty='none', already pruned.\n",
    "             # This block is technically redundant due to pruning check but kept for clarity.\n",
    "             print(\"Error condition: liblinear with penalty='none'. Should have been pruned.\")\n",
    "             return float('inf') # Should not happen\n",
    "\n",
    "        model_fold_lr = LogisticRegression(**lr_model_params)\n",
    "        model_fold_lr.fit(X_train, y_train)\n",
    "\n",
    "        # --- Evaluation ---\n",
    "        train_predictions = model_fold_lr.predict(X_train)\n",
    "        train_accuracy += accuracy_score(y_train, train_predictions)\n",
    "\n",
    "        validation_predictions = model_fold_lr.predict(X_val)\n",
    "        validation_accuracy += accuracy_score(y_val, validation_predictions)\n",
    "\n",
    "        fold_training_accuracies.append(accuracy_score(y_train, train_predictions))\n",
    "        fold_validation_accuracies.append(accuracy_score(y_val, validation_predictions))\n",
    "\n",
    "        # Use try-except for confusion matrix if a class might be missing in y_val\n",
    "        try:\n",
    "            tn, fp, fn, tp = confusion_matrix(y_val, validation_predictions, labels=[0, 1]).ravel()\n",
    "             # Calculate sensitivity (True Positive Rate for class 1)\n",
    "            sensitivity = tp / (tp + fn) if (tp + fn) != 0 else 0\n",
    "            # Calculate specificity (True Negative Rate for class 0)\n",
    "            specificity = tn / (tn + fp) if (tn + fp) != 0 else 0\n",
    "            # Calculate G-Mean.\n",
    "            gmean += sqrt(max(0, sensitivity * specificity)) # Ensure non-negative before sqrt\n",
    "        except ValueError:\n",
    "             print(f\"Warning: Could not compute confusion matrix for fold {fold}. Check class distribution in y_val.\")\n",
    "             # Handle G-mean calculation appropriately, e.g., set fold gmean to 0 or skip\n",
    "             gmean += 0 # Or choose another way to handle this fold's contribution\n",
    "\n",
    "    # --- Calculate Average Metrics ---\n",
    "    n_splits = kf.get_n_splits() # Use actual number of splits performed\n",
    "    avg_training_accuracy = train_accuracy / n_splits\n",
    "    avg_validation_accuracy = validation_accuracy / n_splits\n",
    "\n",
    "    # --- Optimization Goal ---\n",
    "    optimization_goal = gmean / 5\n",
    "\n",
    "    # --- Log Trial Data ---\n",
    "    trial_data.append(\n",
    "        {\n",
    "            # PCA/Data params\n",
    "            \"n_components\": param_grid[\"n_components\"],\n",
    "            # --- Logistic Regression params ---\n",
    "            \"solver\": param_grid[\"solver\"],\n",
    "            \"penalty\": param_grid[\"penalty\"],\n",
    "            \"C\": param_grid.get(\"C\", None), # Use .get as C might not exist if penalty='none'\n",
    "            \"max_iter\": param_grid[\"max_iter\"],\n",
    "            \"class_weight\": param_grid[\"class_weight\"],\n",
    "            # Metrics\n",
    "            \"avg_training_accuracy\": avg_training_accuracy,\n",
    "            \"avg_validation_accuracy\": avg_validation_accuracy,\n",
    "            \"avg_gmean\": gmean / 5,\n",
    "            \"optimization_goal\": optimization_goal\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return optimization_goal\n"
   ],
   "id": "af7b2d1bdb7db5fb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Support Vector Machine",
   "id": "ed4c1faf1d46d402"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.svm import SVC # Import Support Vector Classifier\n",
    "\n",
    "# Optuna optimization function using SVM\n",
    "def objective_svm(trial, features, labels, trial_data):\n",
    "\n",
    "    # print('------------------------------------------------------------------------')\n",
    "    # print(f'Training model number: {trial.number} ...') # Use trial.number\n",
    "\n",
    "    # --- Support Vector Machine (SVC) Hyperparameter Grid ---\n",
    "    param_grid = {\n",
    "        # PCA/Data selection params\n",
    "        \"n_components\": trial.suggest_categorical(\"n_components\", [0, 10, 11, 12, 13, 14, 15, 20, 25]), # Use features.shape[1] for max components\n",
    "\n",
    "        # --- SVC specific params ---\n",
    "        \"kernel\": trial.suggest_categorical(\"kernel\", [\"linear\", \"rbf\", \"poly\", \"sigmoid\"]),\n",
    "        \"C\": trial.suggest_float(\"C\", 1e-3, 1e3, log=True), # Regularization parameter\n",
    "        \"probability\": trial.suggest_categorical(\"probability\", [False, True]), # Enable probability estimates (slower)\n",
    "        \"class_weight\": trial.suggest_categorical(\"class_weight\", [None, \"balanced\"]),\n",
    "        # Conditional gamma (only for rbf, poly, sigmoid)\n",
    "        \"gamma_type\": trial.suggest_categorical(\"gamma_type\", [\"scale\", \"auto\", \"float\"]), # How gamma is determined\n",
    "        # Conditional degree (only for poly)\n",
    "        \"degree\": trial.suggest_int(\"degree\", 2, 5), # Degree for poly kernel\n",
    "    }\n",
    "\n",
    "    # --- Determine actual gamma value ---\n",
    "    actual_gamma = None\n",
    "    gamma_float_value = None # To store the suggested float value for logging\n",
    "    if param_grid[\"kernel\"] in [\"rbf\", \"poly\", \"sigmoid\"]:\n",
    "        if param_grid[\"gamma_type\"] == \"float\":\n",
    "            # Suggest gamma value only if type is float and kernel needs it\n",
    "            gamma_float_value = trial.suggest_float(\"gamma_float\", 1e-4, 1e1, log=True)\n",
    "            actual_gamma = gamma_float_value\n",
    "        else:\n",
    "            actual_gamma = param_grid[\"gamma_type\"] # Use 'scale' or 'auto' string\n",
    "\n",
    "    # --- Data Preparation (PCA and Sampling - kept from original) ---\n",
    "    if param_grid['n_components'] == 0:\n",
    "        training_features_pca = features.to_numpy() # Assuming features is a DataFrame\n",
    "        training_labels = np.array(labels)\n",
    "    else:\n",
    "        # Prepare PCA features\n",
    "        pca_svm_model = PCA(n_components=param_grid['n_components'], random_state=42)\n",
    "\n",
    "        # PCA Fit & Transform\n",
    "        # Ensure features is suitable for PCA (e.g., numerical, scaled if necessary)\n",
    "        pca_svm_model.fit(features)\n",
    "        training_features_pca = pca_svm_model.transform(features)\n",
    "        training_labels = np.array(labels)\n",
    "\n",
    "    # --- Cross-Validation and svm_model Training ---\n",
    "    train_accuracy = 0\n",
    "    validation_accuracy = 0\n",
    "    balanced_accuracy = 0\n",
    "    gmean = 0\n",
    "    fold_training_accuracies = []\n",
    "    fold_validation_accuracies = []\n",
    "    fold_lgg_correct = [] # Assuming LGG is class 1\n",
    "\n",
    "    kf = StratifiedShuffleSplit(n_splits=5, test_size=0.25, random_state=42)\n",
    "    for fold, (train_index, val_index) in enumerate(kf.split(training_features_pca, training_labels)):\n",
    "        X_train, X_val = training_features_pca[train_index], training_features_pca[val_index]\n",
    "        y_train, y_val = training_labels[train_index], training_labels[val_index]\n",
    "\n",
    "        # --- Instantiate and Train SVC svm_model ---\n",
    "        svm_model_params = {\n",
    "            \"C\": param_grid['C'],\n",
    "            \"kernel\": param_grid['kernel'],\n",
    "            \"probability\": param_grid['probability'],\n",
    "            \"class_weight\": param_grid['class_weight'],\n",
    "            \"random_state\": 42,\n",
    "            # Consider adding cache_size if memory/speed becomes an issue\n",
    "            # \"cache_size\": 500 # Example: 500MB cache\n",
    "        }\n",
    "        # Add gamma only if kernel requires it\n",
    "        if param_grid[\"kernel\"] in [\"rbf\", \"poly\", \"sigmoid\"]:\n",
    "            svm_model_params[\"gamma\"] = actual_gamma\n",
    "        # Add degree only if kernel is poly\n",
    "        if param_grid[\"kernel\"] == \"poly\":\n",
    "            svm_model_params[\"degree\"] = param_grid['degree']\n",
    "\n",
    "        svm_model = SVC(**svm_model_params)\n",
    "\n",
    "        svm_model.fit(X_train, y_train)\n",
    "\n",
    "        # --- Evaluation ---\n",
    "        train_predictions = svm_model.predict(X_train)\n",
    "        train_accuracy += accuracy_score(y_train, train_predictions)\n",
    "\n",
    "        validation_predictions = svm_model.predict(X_val)\n",
    "        validation_accuracy += accuracy_score(y_val, validation_predictions)\n",
    "\n",
    "        # Calculate correct predictions for class 1 (assuming LGG is class 1)\n",
    "        lgg_indices_in_val = (y_val == 1)\n",
    "        if np.sum(lgg_indices_in_val) > 0: # Avoid division by zero if no class 1 in validation\n",
    "             correct_lgg = np.sum(validation_predictions[lgg_indices_in_val] == 1)\n",
    "             fold_lgg_correct.append(correct_lgg / np.sum(lgg_indices_in_val))\n",
    "        else:\n",
    "             fold_lgg_correct.append(0) # Or handle as appropriate (e.g., NaN, skip fold metric)\n",
    "\n",
    "\n",
    "        fold_training_accuracies.append(accuracy_score(y_train, train_predictions))\n",
    "        fold_validation_accuracies.append(accuracy_score(y_val, validation_predictions))\n",
    "\n",
    "        balanced_accuracy += balanced_accuracy_score(y_val, validation_predictions)\n",
    "\n",
    "        # Use try-except for confusion matrix if a class might be missing in y_val\n",
    "        try:\n",
    "            tn, fp, fn, tp = confusion_matrix(y_val, validation_predictions, labels=[0, 1]).ravel()\n",
    "             # Calculate sensitivity (True Positive Rate for class 1)\n",
    "            sensitivity = tp / (tp + fn) if (tp + fn) != 0 else 0\n",
    "            # Calculate specificity (True Negative Rate for class 0)\n",
    "            specificity = tn / (tn + fp) if (tn + fp) != 0 else 0\n",
    "            # Calculate G-Mean.\n",
    "            gmean += sqrt(max(0, sensitivity * specificity)) # Ensure non-negative before sqrt\n",
    "        except ValueError:\n",
    "             print(f\"Warning: Could not compute confusion matrix for fold {fold}. Check class distribution in y_val.\")\n",
    "             # Handle G-mean calculation appropriately, e.g., set fold gmean to 0 or skip\n",
    "             gmean += 0 # Or choose another way to handle this fold's contribution\n",
    "\n",
    "    # --- Calculate Average Metrics ---\n",
    "    n_splits = kf.get_n_splits() # Use actual number of splits performed\n",
    "    avg_training_accuracy = train_accuracy / n_splits\n",
    "    avg_validation_accuracy = validation_accuracy / n_splits\n",
    "\n",
    "    # --- optimization_goal ---\n",
    "    optimization_goal = gmean / 5\n",
    "\n",
    "    # --- Log Trial Data ---\n",
    "    trial_data.append({\n",
    "            # PCA/Data params\n",
    "            \"n_components\": param_grid[\"n_components\"],\n",
    "            # --- SVC params ---\n",
    "            \"kernel\": param_grid[\"kernel\"],\n",
    "            \"C\": param_grid[\"C\"],\n",
    "            \"probability\": param_grid[\"probability\"],\n",
    "            \"class_weight\": param_grid[\"class_weight\"],\n",
    "            \"gamma_type\": param_grid[\"gamma_type\"], # Log how gamma was chosen\n",
    "            \"gamma\": param_grid.get(\"actual_gamma\", 0),\n",
    "            \"gamma_float\": gamma_float_value,\n",
    "            \"degree\": param_grid['degree'], # Log degree for poly kernel\n",
    "            # Metrics\n",
    "            \"avg_training_accuracy\": avg_training_accuracy,\n",
    "            \"avg_validation_accuracy\": avg_validation_accuracy,\n",
    "            'avg_gmean': gmean / 5,\n",
    "            \"optimization_goal\": optimization_goal\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return optimization_goal\n"
   ],
   "id": "dd30fc14e5b38615",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Train and save the optimal",
   "id": "33ad3e44a33983d2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def train_model_lr(optimal_params, features, labels):\n",
    "    # --- Data Preparation (PCA and Sampling - kept from original) ---\n",
    "    if optimal_params['n_components'] == 0:\n",
    "        pca_model_lr = None\n",
    "        training_features_pca = features.to_numpy() # Assuming features is a DataFrame\n",
    "        training_labels = np.array(labels)\n",
    "    else:\n",
    "        # Prepare PCA features\n",
    "        pca_model_lr = PCA(n_components=optimal_params['n_components'], random_state=42)\n",
    "\n",
    "        # PCA Fit & Transform\n",
    "        # Ensure features is suitable for PCA (e.g., numerical, scaled if necessary)\n",
    "        pca_model_lr.fit(features)\n",
    "        training_features_pca = pca_model_lr.transform(features)\n",
    "        training_labels = np.array(labels)\n",
    "    lr_model_params = {\n",
    "        \"solver\": optimal_params['solver'],\n",
    "        \"penalty\": optimal_params['penalty'],\n",
    "        \"max_iter\": optimal_params['max_iter'],\n",
    "        \"class_weight\": optimal_params['class_weight'],\n",
    "        \"C\": optimal_params.get('C', None), # Use .get as C might not exist if penalty='none''C'],\n",
    "        \"random_state\": 42,\n",
    "        \"n_jobs\": 1 # Use all available CPU cores\n",
    "    }\n",
    "\n",
    "    model_lr = LogisticRegression(**lr_model_params)\n",
    "    model_lr.fit(training_features_pca, training_labels)\n",
    "\n",
    "    return model_lr, pca_model_lr\n",
    "\n"
   ],
   "id": "4e08a6f91521f782",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def train_model_svm(optimal_params, features, labels):\n",
    "    # --- Determine actual gamma value ---\n",
    "    actual_gamma = None\n",
    "    gamma_float_value = None # To store the suggested float value for logging\n",
    "    if optimal_params[\"kernel\"] in [\"rbf\", \"poly\", \"sigmoid\"]:\n",
    "        if optimal_params[\"gamma_type\"] == \"float\":\n",
    "            # Suggest gamma value only if type is float and kernel needs it\n",
    "            gamma_float_value = 0.001 #optimal_params[\"gamma_float\"]\n",
    "            actual_gamma = gamma_float_value\n",
    "        else:\n",
    "            actual_gamma = optimal_params[\"gamma_type\"] # Use 'scale' or 'auto' string\n",
    "\n",
    "    # --- Data Preparation (PCA and Sampling - kept from original) ---\n",
    "    if optimal_params['n_components'] == 0:\n",
    "        pca_svm_model = None\n",
    "        training_features_pca = features.to_numpy() # Assuming features is a DataFrame\n",
    "        training_labels = np.array(labels)\n",
    "    else:\n",
    "        # Prepare PCA features\n",
    "        pca_svm_model = PCA(n_components=optimal_params['n_components'], random_state=42)\n",
    "\n",
    "        # PCA Fit & Transform\n",
    "        # Ensure features is suitable for PCA (e.g., numerical, scaled if necessary)\n",
    "        pca_svm_model.fit(features)\n",
    "        training_features_pca = pca_svm_model.transform(features)\n",
    "        training_labels = np.array(labels)\n",
    "\n",
    "    svm_model_params = {\n",
    "        \"C\": optimal_params['C'],\n",
    "        \"kernel\": optimal_params['kernel'],\n",
    "        \"probability\": optimal_params['probability'],\n",
    "        \"class_weight\": optimal_params['class_weight'],\n",
    "        \"random_state\": 42,\n",
    "        # Consider adding cache_size if memory/speed becomes an issue\n",
    "        # \"cache_size\": 500 # Example: 500MB cache\n",
    "    }\n",
    "    # Add gamma only if kernel requires it\n",
    "    if optimal_params[\"kernel\"] in [\"rbf\", \"poly\", \"sigmoid\"]:\n",
    "        svm_model_params[\"gamma\"] = actual_gamma\n",
    "    # Add degree only if kernel is poly\n",
    "    if optimal_params[\"kernel\"] == \"poly\":\n",
    "        svm_model_params[\"degree\"] = optimal_params['degree']\n",
    "\n",
    "    svm_model = SVC(**svm_model_params)\n",
    "    svm_model.fit(training_features_pca, training_labels)\n",
    "\n",
    "    return svm_model, pca_svm_model"
   ],
   "id": "692deb9e1a81b2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Step 1 - Training Models on PMOC",
   "id": "2fa27f1fce074524"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Hyperparameter tuning for LR for PMOC\n",
    "trial_data_lr_PMOC_step_1 = []\n",
    "study = optuna.create_study(direction=\"maximize\", study_name=\"DNN Classifier\")\n",
    "func = lambda trial: objective_lr(trial, train_features_step_1.reset_index(drop=True), training_anno_step_1['Label_PMOC'], trial_data_lr_PMOC_step_1)\n",
    "optuna.logging.set_verbosity(optuna.logging.FATAL)\n",
    "study.optimize(func, n_trials=1000, show_progress_bar=True, gc_after_trial=True, n_jobs=16)"
   ],
   "id": "66fbb949aed07e5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Hyperparameter tuning for SVM for PMOC\n",
    "trial_data_svm_PMOC_step_1 = []\n",
    "study = optuna.create_study(direction=\"maximize\", study_name=\"DNN Classifier\")\n",
    "func = lambda trial: objective_svm(trial, train_features_step_1.reset_index(drop=True), training_anno_step_1['Label_PMOC'], trial_data_svm_PMOC_step_1)\n",
    "optuna.logging.set_verbosity(optuna.logging.FATAL)\n",
    "study.optimize(func, n_trials=1000, show_progress_bar=True, gc_after_trial=True, n_jobs=16)"
   ],
   "id": "1bf04f6e6fcb38bc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Save trial data",
   "id": "7906cfa5d2ad3d74"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "trial_data_step_1 = {\n",
    "    'LR_PMOC': trial_data_lr_PMOC_step_1,\n",
    "    'SVM_PMOC': trial_data_svm_PMOC_step_1,\n",
    "}\n",
    "with open('data/trials_step_1.pkl', 'wb') as f:\n",
    "    pickle.dump(trial_data_step_1, f)"
   ],
   "id": "b85983e77d55671c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "optimal_models_params_step_1 = []\n",
    "\n",
    "for model in [\n",
    "    'LR_PMOC',\n",
    "    'SVM_PMOC',\n",
    "]:\n",
    "    best = 0\n",
    "    best_trial = None\n",
    "    for trial in trial_data_step_1[model]:\n",
    "\n",
    "        if trial['optimization_goal'] > best:\n",
    "            best = trial['optimization_goal']\n",
    "            best_trial = trial\n",
    "\n",
    "    optimal_models_params_step_1.append({\"model_name\": model, \"params\": best_trial})\n"
   ],
   "id": "59c92ce09a75a8e7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for param in optimal_models_params_step_1:\n",
    "    print(f\"{param['model_name']}: {param['params']}\")\n",
    "\n",
    "    if param['model_name'] == 'LR_PMOC':\n",
    "        lr_model_PMOC_step_1, pca_model_lr_PMOC_step_1 = train_model_lr(param['params'], train_features_step_1, training_anno_step_1['Label_PMOC'])\n",
    "\n",
    "    elif param['model_name'] == 'SVM_PMOC':\n",
    "        svm_model_PMOC_step_1, pca_model_svm_PMOC_step_1 = train_model_svm(param['params'], train_features_step_1, training_anno_step_1['Label_PMOC'])\n"
   ],
   "id": "d97c176ff68fac8b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Save models\n",
    "with open('models/lr_model_PMOC.pkl', 'wb') as f:\n",
    "    pickle.dump(lr_model_PMOC_step_1, f)\n",
    "with open('models/pca_model_lr_PMOC.pkl', 'wb') as f:\n",
    "    pickle.dump(pca_model_lr_PMOC_step_1, f)\n",
    "with open('models/svm_model_PMOC.pkl', 'wb') as f:\n",
    "    pickle.dump(svm_model_PMOC_step_1, f)\n",
    "with open('models/pca_model_svm_PMOC.pkl', 'wb') as f:\n",
    "    pickle.dump(pca_model_svm_PMOC_step_1, f)"
   ],
   "id": "f69f91bbb0f4b15a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Step 2 - Training Models on SMOC",
   "id": "fad5fe4177c6647c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load features and labels",
   "id": "2e3c50faff110c8b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Train models SMOC",
   "id": "79bcb4ec203eb0dd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Hyperparameter tuning for LR for SMOC\n",
    "trial_data_lr_SMOC_step_2 = []\n",
    "study = optuna.create_study(direction=\"maximize\", study_name=\"DNN Classifier\")\n",
    "func = lambda trial: objective_lr(trial, train_features_step_2.reset_index(drop=True), training_anno_step_2['Label_SMOC'], trial_data_lr_SMOC_step_2)\n",
    "optuna.logging.set_verbosity(optuna.logging.FATAL)\n",
    "study.optimize(func, n_trials=1000, show_progress_bar=True, gc_after_trial=True, n_jobs=16)"
   ],
   "id": "2fc81c83d6a32a8c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Hyperparameter tuning for SVM for PMOC\n",
    "trial_data_svm_SMOC_step_2 = []\n",
    "study = optuna.create_study(direction=\"maximize\", study_name=\"DNN Classifier\")\n",
    "func = lambda trial: objective_svm(trial, train_features_step_2.reset_index(drop=True), training_anno_step_2['Label_SMOC'], trial_data_svm_SMOC_step_2)\n",
    "optuna.logging.set_verbosity(optuna.logging.FATAL)\n",
    "study.optimize(func, n_trials=1000, show_progress_bar=True, gc_after_trial=True, n_jobs=16)"
   ],
   "id": "702014a7d4f43477",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "trial_data_step_2 = {\n",
    "    'LR_SMOC': trial_data_lr_SMOC_step_2,\n",
    "    'SVM_SMOC': trial_data_svm_SMOC_step_2,\n",
    "}\n",
    "with open('data/trials_step_2.pkl', 'wb') as f:\n",
    "    pickle.dump(trial_data_step_2, f)"
   ],
   "id": "320004cffc0c703c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "optimal_models_params_step_2 = []\n",
    "\n",
    "for model in [\n",
    "    'LR_SMOC',\n",
    "    'SVM_SMOC',\n",
    "]:\n",
    "    best = 0\n",
    "    best_trial = None\n",
    "    for trial in trial_data_step_2[model]:\n",
    "\n",
    "        if trial['optimization_goal'] > best:\n",
    "            best = trial['optimization_goal']\n",
    "            best_trial = trial\n",
    "\n",
    "    optimal_models_params_step_2.append({\"model_name\": model, \"params\": best_trial})\n"
   ],
   "id": "f05ac30a072a2c03",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for param in optimal_models_params_step_2:\n",
    "    print(f\"{param['model_name']}: {param['params']}\")\n",
    "\n",
    "    if param['model_name'] == 'LR_SMOC':\n",
    "        lr_model_SMOC, pca_model_lr_SMOC = train_model_lr(param['params'], train_features_step_2, training_anno_step_2['Label_SMOC'])\n",
    "\n",
    "    elif param['model_name'] == 'SVM_SMOC':\n",
    "        # print(param['params'])\n",
    "        svm_model_SMOC, pca_model_svm_SMOC = train_model_svm(param['params'], train_features_step_2, training_anno_step_2['Label_SMOC'])\n"
   ],
   "id": "902c645915e71ce3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Save models\n",
    "with open('models/lr_model_SMOC.pkl', 'wb') as f:\n",
    "    pickle.dump(lr_model_SMOC, f)\n",
    "with open('models/pca_model_lr_SMOC.pkl', 'wb') as f:\n",
    "    pickle.dump(pca_model_lr_SMOC, f)\n",
    "with open('models/svm_model_SMOC.pkl', 'wb') as f:\n",
    "    pickle.dump(svm_model_SMOC, f)\n",
    "with open('models/pca_model_svm_SMOC.pkl', 'wb') as f:\n",
    "    pickle.dump(pca_model_svm_SMOC, f)\n"
   ],
   "id": "37f7eb4ad9d8e7cd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Step 3 - SMOC Origin",
   "id": "7e993648ab7c66a5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Train models SMOC Origin",
   "id": "850081a4331ba73b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### STAD",
   "id": "a58a237f412bf550"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Hyperparameter tuning for LR for STAD\n",
    "trial_data_lr_STAD = []\n",
    "study = optuna.create_study(direction=\"maximize\", study_name=\"DNN Classifier\")\n",
    "func = lambda trial: objective_lr(trial, train_features_step_3.reset_index(drop=True), training_anno_step_3['Label_STAD'], trial_data_lr_STAD)\n",
    "optuna.logging.set_verbosity(optuna.logging.FATAL)\n",
    "study.optimize(func, n_trials=1000, show_progress_bar=True, gc_after_trial=True, n_jobs=16)"
   ],
   "id": "216cd6353b297d93",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Hyperparameter tuning for SVM for PMOC\n",
    "trial_data_svm_STAD = []\n",
    "study = optuna.create_study(direction=\"maximize\", study_name=\"DNN Classifier\")\n",
    "func = lambda trial: objective_svm(trial, train_features_step_3.reset_index(drop=True), training_anno_step_3['Label_STAD'], trial_data_svm_STAD)\n",
    "optuna.logging.set_verbosity(optuna.logging.FATAL)\n",
    "study.optimize(func, n_trials=1000, show_progress_bar=True, gc_after_trial=True, n_jobs=16)"
   ],
   "id": "9e919ed8bcf01432",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### COAD",
   "id": "79f4da575b1acf07"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Hyperparameter tuning for LR for COAD\n",
    "trial_data_lr_COAD = []\n",
    "study = optuna.create_study(direction=\"maximize\", study_name=\"DNN Classifier\")\n",
    "func = lambda trial: objective_lr(trial, train_features_step_3.reset_index(drop=True), training_anno_step_3['Label_COAD'], trial_data_lr_COAD)\n",
    "optuna.logging.set_verbosity(optuna.logging.FATAL)\n",
    "study.optimize(func, n_trials=1000, show_progress_bar=True, gc_after_trial=True, n_jobs=16)"
   ],
   "id": "e93b597435439407",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Hyperparameter tuning for SVM for PMOC\n",
    "trial_data_svm_COAD = []\n",
    "study = optuna.create_study(direction=\"maximize\", study_name=\"DNN Classifier\")\n",
    "func = lambda trial: objective_svm(trial, train_features_step_3.reset_index(drop=True), training_anno_step_3['Label_COAD'], trial_data_svm_COAD)\n",
    "optuna.logging.set_verbosity(optuna.logging.FATAL)\n",
    "study.optimize(func, n_trials=100, show_progress_bar=True, gc_after_trial=True, n_jobs=16)"
   ],
   "id": "4eeeb5742a242209",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Save and evaluate models",
   "id": "938be7b862a76904"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "full_trial_data = {\n",
    "    'LR_STAD': trial_data_lr_STAD,\n",
    "    'SVM_STAD': trial_data_svm_STAD,\n",
    "    'LR_COAD': trial_data_lr_COAD,\n",
    "    'SVM_COAD': trial_data_svm_COAD,\n",
    "}\n",
    "with open('data/trials_step_3.pkl', 'wb') as f:\n",
    "    pickle.dump(full_trial_data, f)"
   ],
   "id": "6096ea4ee3f138c2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "optimal_models_params_step_3 = []\n",
    "\n",
    "for model in [\n",
    "    'LR_STAD',\n",
    "    'SVM_STAD',\n",
    "    'LR_COAD',\n",
    "    'SVM_COAD',\n",
    "]:\n",
    "    best = 0\n",
    "    best_trial = None\n",
    "    for trial in full_trial_data[model]:\n",
    "\n",
    "        if trial['optimization_goal'] > best:\n",
    "            best = trial['optimization_goal']\n",
    "            best_trial = trial\n",
    "\n",
    "    optimal_models_params_step_3.append({\"model_name\": model, \"params\": best_trial})\n"
   ],
   "id": "5c22800c17efdf84",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for param in optimal_models_params_step_3:\n",
    "    print(f\"{param['model_name']}: {param['params']}\")\n",
    "\n",
    "    if param['model_name'] == 'LR_STAD':\n",
    "        lr_model_STAD, pca_model_lr_STAD = train_model_lr(param['params'], train_features_step_3, training_anno_step_3['Label_STAD'])\n",
    "\n",
    "    elif param['model_name'] == 'SVM_STAD':\n",
    "        svm_model_STAD, pca_model_svm_STAD = train_model_svm(param['params'], train_features_step_3, training_anno_step_3['Label_STAD'])\n",
    "\n",
    "    elif param['model_name'] == 'LR_COAD':\n",
    "        lr_model_COAD, pca_model_lr_COAD = train_model_lr(param['params'], train_features_step_3, training_anno_step_3['Label_COAD'])\n",
    "\n",
    "    elif param['model_name'] == 'SVM_COAD':\n",
    "        svm_model_COAD, pca_model_svm_COAD = train_model_svm(param['params'], train_features_step_3, training_anno_step_3['Label_COAD'])\n"
   ],
   "id": "7c6726a40d036049",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# # Save models\n",
    "with open('models/lr_model_STAD.pkl', 'wb') as f:\n",
    "    pickle.dump(lr_model_STAD, f)\n",
    "with open('models/pca_model_lr_STAD.pkl', 'wb') as f:\n",
    "    pickle.dump(pca_model_lr_STAD, f)\n",
    "with open('models/svm_model_STAD.pkl', 'wb') as f:\n",
    "    pickle.dump(svm_model_STAD, f)\n",
    "with open('models/pca_model_svm_STAD.pkl', 'wb') as f:\n",
    "    pickle.dump(pca_model_svm_STAD, f)\n",
    "\n",
    "\n",
    "with open('models/lr_model_COAD.pkl', 'wb') as f:\n",
    "    pickle.dump(lr_model_COAD, f)\n",
    "with open('models/pca_model_lr_COAD.pkl', 'wb') as f:\n",
    "    pickle.dump(pca_model_lr_COAD, f)\n",
    "with open('models/svm_model_COAD.pkl', 'wb') as f:\n",
    "    pickle.dump(svm_model_COAD, f)\n",
    "with open('models/pca_model_svm_COAD.pkl', 'wb') as f:\n",
    "    pickle.dump(pca_model_svm_COAD, f)\n"
   ],
   "id": "597c0024a16cafed",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
